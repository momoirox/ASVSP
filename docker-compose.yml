version: '3' 

services:

  namenode:
    image: bde2020/hadoop-namenode:${HADOOP_MASTERNODE_VERSION:-2.0.0-hadoop3.2.1-java8}
    container_name: namenode
    volumes:
      - hadoop_namenode:/hadoop/dfs/name
      - ./Data:/Data
    environment:
      - CLUSTER_NAME=test
    env_file:
      - ./hadoop.env
    ports:
      - 9870:9870
      - 9000:9000

  datanode1:
    image: bde2020/hadoop-datanode:${HADOOP_DATANODE_VERSION:-2.0.0-hadoop3.2.1-java8}
    container_name: datanode1
    depends_on: 
      - namenode
    volumes:
      - hadoop_datanode1:/hadoop/dfs/data
    env_file:
      - ./hadoop.env

  spark-master:
    image: bde2020/spark-master:${SPARK_MASTER_VERSION:-3.0.1-hadoop3.2}
    container_name: spark-master
    ports:
      - 8080:8080
      - 7077:7077
    environment: 
      - PYSPARK_PYTHON=python3
    env_file:
      - ./hadoop.env
    volumes: 
      - ./Producer:/Producer
      - ./Consumer:/Consumer
      - ./Data:/Data
   
  spark-worker1:
    image: bde2020/spark-worker:${SPARK_WORKER_VERSION:-3.0.1-hadoop3.2}
    container_name: spark-worker1
    depends_on:
      - spark-master
    environment:
      - SPARK_MASTER=spark://spark-master:7077
    ports:
      - 8081:8081
    env_file:
      - ./hadoop.env
  
  hue:
    image: gethue/hue:20201111-135001
    hostname: hue
    container_name: hue
    dns: 8.8.8.8
    ports:
      - "8888:8888"
    volumes:
      - ./conf.dist:/usr/share/hue/desktop/conf
    depends_on:
      - namenode

  db:
    image: postgres
    restart: always
    container_name: postgres
    environment:
      - POSTGRES_PASSWORD=postgres
      - POSTGRES_USER=postgres
      - POSTGRES_DB=postgres
    ports:
      - 5432:5432
    volumes:
      - db-data:/var/lib/postgresql/data

  pgadmin:
    image: dpage/pgadmin4
    ports:
      - "5050:80"
    environment:
      PGADMIN_DEFAULT_EMAIL: momoirox@gmail.com
      PGADMIN_DEFAULT_PASSWORD: password
    
  metabase:
    volumes:
      - metabase-data:/metabase-data
    environment:
      - MB_DB_FILE=/metabase-data/metabase.db
      - MB_DB_TYPE=postgres
      - MB_DB_DBNAME=postgres
      - MB_DB_PORT=5432
      - MB_DB_USER=postgres
      - MB_DB_PASS=postgres
      - MB_DB_HOST=db
      - PGDATA= /var/lib/postgresql/data/init/
    ports:
        - 3000:3000
    image: metabase/metabase
    restart: always
    depends_on: 
      - db

  zoo1:
    image: zookeeper:3.4.9
    container_name: zoo1
    ports:
      - "2181:2181"
    environment:
        ZOO_MY_ID: 1
        ZOO_PORT: 2181
        ZOO_SERVERS: server.1=zoo1:2888:3888
    volumes:
      - zoo1:/data
      - zoo1log:/datalog 

  kafka1:
    image: confluentinc/cp-kafka:5.5.12
    container_name: kafka1
    ports:
      - "9092:9092"
    restart: always
    environment:
      KAFKA_ADVERTISED_LISTENERS: LISTENER_DOCKER_INTERNAL://kafka1:19092,LISTENER_DOCKER_EXTERNAL://${DOCKER_HOST_IP:-127.0.0.1}:9092
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: LISTENER_DOCKER_INTERNAL:PLAINTEXT,LISTENER_DOCKER_EXTERNAL:PLAINTEXT
      KAFKA_INTER_BROKER_LISTENER_NAME: LISTENER_DOCKER_INTERNAL
      KAFKA_ZOOKEEPER_CONNECT: "zoo1:2181"
      KAFKA_BROKER_ID: 1
      KAFKA_LOG4J_LOGGERS: "kafka.controller=INFO,kafka.producer.async.DefaultEventHandler=INFO,state.change.logger=INFO"
      KAFKA_AUTO_CREATE_TOPICS_ENABLE: "true"
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1
    volumes:
      - kafka1:/var/lib/kafka/data
    depends_on:
      - zoo1

  kafka2:
    image: confluentinc/cp-kafka:5.5.12
    container_name: kafka2
    ports:
      - "9093:9093"
    restart: always
    environment:
      KAFKA_ADVERTISED_LISTENERS: LISTENER_DOCKER_INTERNAL://kafka2:19093,LISTENER_DOCKER_EXTERNAL://${DOCKER_HOST_IP:-127.0.0.1}:9093
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: LISTENER_DOCKER_INTERNAL:PLAINTEXT,LISTENER_DOCKER_EXTERNAL:PLAINTEXT
      KAFKA_INTER_BROKER_LISTENER_NAME: LISTENER_DOCKER_INTERNAL
      KAFKA_ZOOKEEPER_CONNECT: "zoo1:2181"
      KAFKA_BROKER_ID: 2
      KAFKA_LOG4J_LOGGERS: "kafka.controller=INFO,kafka.producer.async.DefaultEventHandler=INFO,state.change.logger=INFO"
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1
    volumes:
      - kafka2:/var/lib/kafka/data
    depends_on:
      - zoo1

  producer:
    build: 
      context: ./Producer/Real
    container_name: kafka_producer_1
    restart: unless-stopped
    mem_limit: 200M
    volumes:
      - ./Producer/Real:/app:ro
    environment:
      - KAFKA_BROKER=kafka1:19092
      - KAFKA_TOPIC=tmdb-movies
      - DATASET_API_LINK=https://api.themoviedb.org/3/
    depends_on:
      - kafka1
    env_file:
      - ./hadoop.env

  init-kafka:
      image: confluentinc/cp-kafka:6.1.1
      depends_on:
        - kafka1
        - kafka2
      entrypoint: [ '/bin/sh', '-c' ]
      command: |
        "
        # blocks until kafka is reachable
        echo -e 'Creating kafka topics'
        kafka-topics --bootstrap-server kafka1:19092 --create --if-not-exists --topic tmdb-movies --replication-factor 1 --partitions 1
        echo -e 'Successfully created the following topics:'
        kafka-topics --bootstrap-server kafka1:19092 --list
        kafka-topics --bootstrap-server kafka2:19093 --list
        "
      
volumes:
  hadoop_namenode:
  hadoop_datanode1:
  db-data:
  metabase-data:
  zoo1:
  zoo1log:
  kafka1:
  kafka2:
